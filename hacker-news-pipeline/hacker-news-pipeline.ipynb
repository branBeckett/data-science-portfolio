{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "In this project, we will build a data pipeline from scratch and apply it to data from [Hacker News](https://news.ycombinator.com). The pipeline will filter, clean, aggregate, and summarize data from a JSON API in a sequence of functions that will apply the transformations for us.\n",
    "\n",
    "Our goal will be to find the top 100 keywords of all Hacker News posts in 2014. This will give us a better understanding of the most popular tech topics in 2014.\n",
    "\n",
    "## Introduction to the Data\n",
    "\n",
    "The data has already been downloaded as a list of JSON posts that can be found in the `hn_stories_2014.json` file in this repository.\n",
    "\n",
    "We'll be using the following keys in our data:\n",
    "\n",
    "* `created_at`: Timestamp of the post's creation time.\n",
    "* `created_at_i`: Unix epoch timestamp.\n",
    "* `url`: URL of the post link.\n",
    "* `objectID`: ID of the post.\n",
    "* `author`: Post's author.\n",
    "* `points`: Number of upvotes the post had.\n",
    "* `title`: Headline of the post.\n",
    "* `num_comments`: Number of comments on the post.\n",
    "\n",
    "Let's start by instantiating our pipeline class and importing the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import io\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "from pipeline import Pipeline\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "Next, we'll load the JSON data into Python. Since JSON files resemble a key-value dictionary, we'll parse the data into a Python dict object using the `json` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories\n",
    "\n",
    "We can start working on our data now that the stories have been loaded as a list of dict objects. We'll start by filtering the list of stories to extract the most popular stories in 2014.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `filter_stories()` that will be dependent on the `file_to_json()` function output, and it will return stories that have more than 50 points, at least 1 comment, and do not begin with \"Ask HN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "Now that we've reduced our set of stories, we can write these dict objects to a CSV file so we can have a consistent data format.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `json_to_csv()` that will be depended on the `filter_stories()` output, and it will output the formated input to a CSV using the `IO` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], '%Y-%m%dT%H:%M:%SZ'), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'point', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "\n",
    "Next, we'll create a `pipeline.task()` function called `extract_titles()` that will be dependent on the `json_to_csv()` output, and it will return a generator of every Hacker News story title. After we have all of the titles, we'll be able to count the word frequency which will help us find the most popular topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index_num = header.index('title')\n",
    "    \n",
    "    return (line[index_num] for line in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles\n",
    "\n",
    "In order to have a consistent set of words to use for our word frequency model, we'll need to clean our list of titles and make sure they're all lowercase and without punctuation. The easiest way to remove punctuation from a string is to check each character and only keep the letters. For this, we can use `string.punctuation` to help us.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `clean_title()` that will be dependent on the `extract_titles()` output, and it will return the cleaned titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(t for t in title if t not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    freq_count = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    freq = [\n",
    "        (word, word_freq[world])\n",
    "        for word in sorted(word_freq, key=word_freq.get)\n",
    "    ]\n",
    "    return freq[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this project we built a data pipeline from scratch.\n",
    "\n",
    "The idea for this project comes from the [DATAQUEST](https://app.dataquest.io/) **Building a Data Pipeline** course. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
