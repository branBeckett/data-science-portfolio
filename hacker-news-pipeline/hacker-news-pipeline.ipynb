{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hacker News Pipeline\n",
    "\n",
    "In this project, we will build a data pipeline from scratch and apply it to data from [Hacker News](https://news.ycombinator.com). The pipeline will filter, clean, aggregate, and summarize data from a JSON API in a sequence of functions that will apply the transformations for us.\n",
    "\n",
    "Our goal will be to find the top 100 keywords of all Hacker News posts in 2014. This will give us a better understanding of the most popular tech topics in 2014.\n",
    "\n",
    "## Introduction to the Data\n",
    "\n",
    "The data has already been downloaded as a list of JSON posts that can be found in the `hn_stories_2014.json` file in this repository.\n",
    "\n",
    "We'll be using the following keys in our data:\n",
    "\n",
    "* `created_at`: Timestamp of the post's creation time.\n",
    "* `created_at_i`: Unix epoch timestamp.\n",
    "* `url`: URL of the post link.\n",
    "* `objectID`: ID of the post.\n",
    "* `author`: Post's author.\n",
    "* `points`: Number of upvotes the post had.\n",
    "* `title`: Headline of the post.\n",
    "* `num_comments`: Number of comments on the post.\n",
    "\n",
    "Let's start by instantiating our pipeline class and importing the libraries we'll be using."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "import json\n",
    "import io\n",
    "import string\n",
    "from datetime import datetime\n",
    "\n",
    "from pipeline import Pipeline, build_csv\n",
    "from stop_words import stop_words\n",
    "\n",
    "pipeline = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the JSON Data\n",
    "\n",
    "Next, we'll load the JSON data into Python. Since JSON files resemble a key-value dictionary, we'll parse the data into a Python dict object using the `json` module. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task()\n",
    "def file_to_json():\n",
    "    with open('hn_stories_2014.json', 'r') as file:\n",
    "        data = json.load(file)\n",
    "        stories = data['stories']\n",
    "    return stories"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering the Stories\n",
    "\n",
    "We can start working on our data now that the stories have been loaded as a list of dict objects. We'll start by filtering the list of stories to extract the most popular stories in 2014.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `filter_stories()` that will be dependent on the `file_to_json()` function output, and it will return stories that have more than 50 points, at least 1 comment, and do not begin with \"Ask HN\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=file_to_json)\n",
    "def filter_stories(stories):\n",
    "    def is_popular(story):\n",
    "        return story['points'] > 50 and story['num_comments'] > 1 and not story['title'].startswith('Ask HN')\n",
    "    \n",
    "    return (\n",
    "        story for story in stories\n",
    "        if is_popular(story)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to CSV\n",
    "\n",
    "Now that we've reduced our set of stories, we can write these dict objects to a CSV file so we can have a consistent data format.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `json_to_csv()` that will be depended on the `filter_stories()` output, and it will output the formated input to a CSV using the `IO` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=filter_stories)\n",
    "def json_to_csv(stories):\n",
    "    lines = []\n",
    "    for story in stories:\n",
    "        lines.append(\n",
    "            (story['objectID'], datetime.strptime(story['created_at'], '%Y-%m-%dT%H:%M:%SZ'), story['url'], story['points'], story['title'])\n",
    "        )\n",
    "    return build_csv(lines, header=['objectID', 'created_at', 'url', 'point', 'title'], file=io.StringIO())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Title Column\n",
    "\n",
    "Next, we'll create a `pipeline.task()` function called `extract_titles()` that will be dependent on the `json_to_csv()` output, and it will return a generator of every Hacker News story title. After we have all of the titles, we'll be able to count the word frequency which will help us find the most popular topics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=json_to_csv)\n",
    "def extract_titles(csv_file):\n",
    "    reader = csv.reader(csv_file)\n",
    "    header = next(reader)\n",
    "    index_num = header.index('title')\n",
    "    \n",
    "    return (line[index_num] for line in reader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean the Titles\n",
    "\n",
    "In order to have a consistent set of words to use for our word frequency model, we'll need to clean our list of titles and make sure they're all lowercase and without punctuation. The easiest way to remove punctuation from a string is to check each character and only keep the letters. For this, we can use `string.punctuation` to help us.\n",
    "\n",
    "We'll create a `pipeline.task()` function called `clean_title()` that will be dependent on the `extract_titles()` output, and it will return the cleaned titles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=extract_titles)\n",
    "def clean_title(titles):\n",
    "    for title in titles:\n",
    "        title = title.lower()\n",
    "        title = ''.join(t for t in title if t not in string.punctuation)\n",
    "        yield title"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Frequency Dictionary\n",
    "\n",
    "Now that the data is clean, we can build a word frequency dictionary.\n",
    "\n",
    "We'll want our frequency dictionary to show us only keywords and not all of the stop words that are frequently used and not useful to us for this analysis. To remove these words, we'll use a module in this repository called `stop_words.py` that contains a tuple of all the words we don't need."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=clean_title)\n",
    "def build_keyword_dictionary(titles):\n",
    "    freq_count = {}\n",
    "    for title in titles:\n",
    "        for word in title.split(' '):\n",
    "            if word and word not in stop_words: # Removes the stop words located in our tuple\n",
    "                if word not in freq_count:\n",
    "                    freq_count[word] = 1\n",
    "                freq_count[word] += 1\n",
    "    return freq_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sort the Top Words\n",
    "\n",
    "Finally, we can now sort out the top words. We'll output a list of the top 100 tuples sorted from most used to least used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pipeline.task(depends_on=build_keyword_dictionary)\n",
    "def top_keywords(word_freq):\n",
    "    freq = [\n",
    "        (word, word_freq[word])\n",
    "        for word in sorted(word_freq, key=word_freq.get, reverse=True)\n",
    "    ]\n",
    "    return freq[:100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('new', 186), ('google', 168), ('bitcoin', 102), ('open', 93), ('programming', 91), ('web', 89), ('data', 86), ('video', 80), ('python', 76), ('code', 73), ('facebook', 72), ('released', 72), ('using', 71), ('2013', 66), ('javascript', 66), ('free', 65), ('source', 65), ('game', 64), ('internet', 63), ('microsoft', 60), ('c', 60), ('linux', 59), ('app', 58), ('pdf', 56), ('work', 55), ('language', 55), ('software', 53), ('2014', 53), ('startup', 52), ('apple', 51), ('use', 51), ('make', 51), ('time', 49), ('yc', 49), ('security', 49), ('nsa', 46), ('github', 46), ('windows', 45), ('world', 42), ('way', 42), ('like', 42), ('1', 41), ('project', 41), ('computer', 41), ('heartbleed', 41), ('git', 38), ('users', 38), ('dont', 38), ('design', 38), ('ios', 38), ('developer', 37), ('os', 37), ('twitter', 37), ('ceo', 37), ('vs', 37), ('life', 37), ('big', 36), ('day', 36), ('android', 35), ('online', 35), ('years', 34), ('simple', 34), ('court', 34), ('guide', 33), ('learning', 33), ('mt', 33), ('api', 33), ('says', 33), ('apps', 33), ('browser', 33), ('server', 32), ('firefox', 32), ('fast', 32), ('gox', 32), ('problem', 32), ('mozilla', 32), ('engine', 32), ('site', 32), ('introducing', 31), ('amazon', 31), ('year', 31), ('support', 30), ('stop', 30), ('built', 30), ('better', 30), ('million', 30), ('people', 30), ('text', 30), ('3', 29), ('does', 29), ('tech', 29), ('development', 29), ('billion', 28), ('developers', 28), ('just', 28), ('library', 28), ('did', 28), ('website', 28), ('money', 28), ('inside', 28)]\n"
     ]
    }
   ],
   "source": [
    "run_pipeline = pipeline.run()\n",
    "print(run_pipeline[top_keywords])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion and Next Steps\n",
    "\n",
    "In this project, we built a data pipeline from scratch. We're able to see some interesting topics that were trending in 2014 like bitcoin. \n",
    "\n",
    "To continue our analysis, some next steps we could take to improve this pipeline are:\n",
    "\n",
    "* Before filtering the data, first convert it to a CSV to keep the stories in a raw file.\n",
    "* Acquire the data from Hacker News directly from a JSON API to process newer data.\n",
    "* Use the nltk package for more advanced natural language processing.\n",
    "* Rewrite the Pipeline class so that output is saved to a file after each task so that tasks don't need to be run more than once.\n",
    "\n",
    "The idea for this project comes from the [DATAQUEST](https://app.dataquest.io/) **Building a Data Pipeline** course. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
